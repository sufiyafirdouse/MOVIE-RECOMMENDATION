# -*- coding: utf-8 -*-
"""sorce_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/pavi2006thara/Pavi-naan-muthalvan-/blob/main/sorce_code.ipynb
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

df = pd.read_csv("assessments.csv")
print(df.head())

# ===============================
# Step 1: Import Libraries
# ===============================
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# ===============================
# Step 2: Load the Dataset
# ===============================
df = pd.read_csv("assessments.csv")

# ===============================
# Step 3: Data Cleaning
# ===============================
# Show basic info
print(df.info())
print(df.describe())
print(df.isnull().sum())

# Drop rows with too many missing values (optional threshold)
df = df.dropna(thresh=int(df.shape[1] * 0.6))

# Fill missing numeric values with median
for col in df.select_dtypes(include=np.number).columns:
    df[col].fillna(df[col].median(), inplace=True)

# Fill missing categorical values with mode
for col in df.select_dtypes(include='object').columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

# ===============================
# Step 4: Exploratory Data Analysis (EDA)
# ===============================
# Visualize target distribution (adjust target column name)
if 'target' in df.columns:
    sns.countplot(x='target', data=df)
    plt.title('Target Variable Distribution')
    plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.select_dtypes(include=np.number).corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation")
plt.show()

# Pair plot (optional, slow for large data)
# sns.pairplot(df.select_dtypes(include=np.number))

# ===============================
# Step 5: Feature Engineering
# ===============================
# Encode categorical columns
le = LabelEncoder()
for col in df.select_dtypes(include='object').columns:
    df[col] = le.fit_transform(df[col])

# Feature-target split (adjust target column name)
X = df.drop(columns=['target']) if 'target' in df.columns else df.iloc[:, :-1]
y = df['target'] if 'target' in df.columns else df.iloc[:, -1]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

import joblib
joblib.dump(model, 'rf_model.pkl')

# ===============================
# Step 6: Model Engineering
# ===============================
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Convert target variable to discrete if necessary
# Check if y contains continuous values
if pd.api.types.is_numeric_dtype(y):
    # If continuous, convert to discrete using a suitable method
    # For example, you can use binning or thresholding
    # Here, we'll use a simple threshold to create two classes
    y_train = (y_train > y_train.mean()).astype(int)
    y_test = (y_test > y_test.mean()).astype(int)

# Train a Random Forest model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluation
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ===============================
# Step 7: Save Model (Optional)
# ===============================
import joblib
joblib.dump(model, 'rf_model.pkl')